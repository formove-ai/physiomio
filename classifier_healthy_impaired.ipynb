{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb39af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physiological Recording Classification: Healthy vs Impaired\n",
    "# Simple Random Forest classifier with patient-based splits\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 123\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the normalized data tables\n",
    "print(\"Loading normalized data tables...\")\n",
    "\n",
    "# Load PSD and SNR normalized data\n",
    "psd_data = pd.read_csv('detailed_psd_normalized_table.csv')\n",
    "snr_data = pd.read_csv('detailed_snrs_normalized_table.csv')\n",
    "\n",
    "print(f\"PSD data shape: {psd_data.shape}\")\n",
    "print(f\"SNR data shape: {snr_data.shape}\")\n",
    "print(f\"\\nLabel distribution in PSD data:\")\n",
    "print(psd_data['Arm Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets and prepare features\n",
    "print(\"Combining datasets...\")\n",
    "\n",
    "# Merge PSD and SNR data on Patient, Recording, and Arm Type\n",
    "combined_data = pd.merge(\n",
    "    psd_data, \n",
    "    snr_data, \n",
    "    on=['Patient', 'Recording', 'Arm Type'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Combined data shape: {combined_data.shape}\")\n",
    "print(f\"Combined label distribution:\")\n",
    "print(combined_data['Arm Type'].value_counts())\n",
    "\n",
    "# Prepare features and labels\n",
    "feature_cols = [col for col in combined_data.columns \n",
    "               if col not in ['Patient', 'Recording', 'Arm Type']]\n",
    "\n",
    "X = combined_data[feature_cols].values\n",
    "y = combined_data['Arm Type'].values\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"PSD features: {len([c for c in feature_cols if 'PSD' in c])}\")\n",
    "print(f\"SNR features: {len([c for c in feature_cols if 'SNR' in c])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patient-based train/test split function\n",
    "def patient_based_split(combined_data, X, y, test_size=0.15, random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Split data ensuring no patient appears in both train and test sets.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get patient recording counts\n",
    "    patient_data = combined_data.groupby('Patient').agg({\n",
    "        'Recording': 'count',\n",
    "        'Arm Type': lambda x: list(x)\n",
    "    }).rename(columns={'Recording': 'num_recordings', 'Arm Type': 'arm_types'})\n",
    "    \n",
    "    # Calculate target test size\n",
    "    total_recordings = len(combined_data)\n",
    "    target_test_recordings = int(total_recordings * test_size)\n",
    "    \n",
    "    # Select patients for test set using greedy approach\n",
    "    patients_by_size = patient_data.sort_values('num_recordings', ascending=False)\n",
    "    remaining_patients = list(patients_by_size.index)\n",
    "    \n",
    "    best_test_patients = []\n",
    "    best_diff = float('inf')\n",
    "    \n",
    "    # Try multiple random combinations to find best split\n",
    "    for _ in range(100):\n",
    "        np.random.shuffle(remaining_patients)\n",
    "        current_test_patients = []\n",
    "        current_count = 0\n",
    "        \n",
    "        for patient in remaining_patients:\n",
    "            patient_recordings = patient_data.loc[patient, 'num_recordings']\n",
    "            if current_count + patient_recordings <= target_test_recordings + 5:\n",
    "                current_test_patients.append(patient)\n",
    "                current_count += patient_recordings\n",
    "                \n",
    "                if abs(current_count - target_test_recordings) < abs(best_diff):\n",
    "                    best_test_patients = current_test_patients.copy()\n",
    "                    best_diff = current_count - target_test_recordings\n",
    "    \n",
    "    test_patients = best_test_patients\n",
    "    test_recordings_count = sum(patient_data.loc[p, 'num_recordings'] for p in test_patients)\n",
    "    \n",
    "    print(f\"Selected {len(test_patients)} patients for test set\")\n",
    "    print(f\"Test recordings: {test_recordings_count} ({test_recordings_count/total_recordings*100:.1f}%)\")\n",
    "    \n",
    "    # Create train/test masks\n",
    "    test_mask = combined_data['Patient'].isin(test_patients)\n",
    "    train_mask = ~test_mask\n",
    "    \n",
    "    # Split the data\n",
    "    X_train = X[train_mask]\n",
    "    X_test = X[test_mask]\n",
    "    y_train = y[train_mask]\n",
    "    y_test = y[test_mask]\n",
    "    \n",
    "    # Verify no patient overlap\n",
    "    train_patients = set(combined_data[train_mask]['Patient'].unique())\n",
    "    test_patients_actual = set(combined_data[test_mask]['Patient'].unique())\n",
    "    overlap = train_patients.intersection(test_patients_actual)\n",
    "    \n",
    "    if overlap:\n",
    "        print(f\"WARNING: Patient overlap detected: {overlap}\")\n",
    "    else:\n",
    "        print(\"✓ No patient overlap between train and test sets\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Perform the split\n",
    "print(\"Performing patient-based train/test split...\")\n",
    "X_train, X_test, y_train, y_test = patient_based_split(combined_data, X, y)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training label distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  {label}: {count} ({count/len(y_train)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation function\n",
    "def create_rf_model():\n",
    "    \"\"\"Create a RandomForestClassifier with consistent parameters.\"\"\"\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "\n",
    "# Scale features and train Random Forest\n",
    "print(\"Scaling features and training Random Forest...\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train Random Forest\n",
    "rf_model = create_rf_model()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e09057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation with patient-based splits\n",
    "def patient_based_cv_split(combined_data, n_splits=5, random_state=RANDOM_STATE):\n",
    "    \"\"\"\n",
    "    Create cross-validation splits ensuring no patient appears in multiple folds.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get unique patients\n",
    "    unique_patients = combined_data['Patient'].unique()\n",
    "    np.random.shuffle(unique_patients)\n",
    "    \n",
    "    # Split patients into folds\n",
    "    fold_size = len(unique_patients) // n_splits\n",
    "    patient_folds = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * fold_size\n",
    "        if i == n_splits - 1:  # Last fold gets remaining patients\n",
    "            end_idx = len(unique_patients)\n",
    "        else:\n",
    "            end_idx = (i + 1) * fold_size\n",
    "        patient_folds.append(unique_patients[start_idx:end_idx])\n",
    "    \n",
    "    # Create train/validation indices for each fold\n",
    "    cv_splits = []\n",
    "    for i in range(n_splits):\n",
    "        val_patients = patient_folds[i]\n",
    "        val_mask = combined_data['Patient'].isin(val_patients)\n",
    "        train_mask = ~val_mask\n",
    "        \n",
    "        train_indices = combined_data[train_mask].index.values\n",
    "        val_indices = combined_data[val_mask].index.values\n",
    "        \n",
    "        cv_splits.append((train_indices, val_indices))\n",
    "    \n",
    "    return cv_splits\n",
    "\n",
    "print(\"Performing cross-validation with patient-based splits...\")\n",
    "\n",
    "# Create patient-based CV splits\n",
    "cv_splits = patient_based_cv_split(combined_data, n_splits=5)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = []\n",
    "for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
    "    # Get train and validation data for this fold\n",
    "    X_fold_train = X[train_idx]\n",
    "    X_fold_val = X[val_idx]\n",
    "    y_fold_train = y[train_idx]\n",
    "    y_fold_val = y[val_idx]\n",
    "    \n",
    "    # Scale features\n",
    "    fold_scaler = StandardScaler()\n",
    "    X_fold_train_scaled = fold_scaler.fit_transform(X_fold_train)\n",
    "    X_fold_val_scaled = fold_scaler.transform(X_fold_val)\n",
    "    \n",
    "    # Train model\n",
    "    fold_model = create_rf_model()\n",
    "    fold_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    fold_pred = fold_model.predict(X_fold_val_scaled)\n",
    "    fold_accuracy = accuracy_score(y_fold_val, fold_pred)\n",
    "    cv_scores.append(fold_accuracy)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}: {fold_accuracy:.3f}\")\n",
    "\n",
    "cv_mean = np.mean(cv_scores)\n",
    "cv_std = np.std(cv_scores)\n",
    "\n",
    "print(f\"\\nCross-Validation Results:\")\n",
    "print(f\"Mean CV Accuracy: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
    "print(f\"Individual fold scores: {[f'{score:.3f}' for score in cv_scores]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008486c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for test set\n",
    "print(\"Plotting confusion matrix...\")\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Healthy', 'Impaired'], \n",
    "           yticklabels=['Healthy', 'Impaired'])\n",
    "plt.title(f'Confusion Matrix - Random Forest\\nTest Accuracy: {test_accuracy:.3f}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"========\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"Cross-Validation Accuracy: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "print(f\"Features used: {X.shape[1]} (PSD + SNR normalized features)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0efa01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
