{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80186e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Physiological Recording Classification: Healthy vs Impaired\n",
    "==========================================================\n",
    "\n",
    "This script combines PSD and SNR features to build classifiers that distinguish\n",
    "between healthy and impaired patient recordings.\n",
    "\n",
    "Features:\n",
    "- Loads and combines PSD and SNR feature tables\n",
    "- Implements customizable filtering via filter functions\n",
    "- Implements multiple simple classifiers (Decision Tree, Random Forest, SVM, etc.)\n",
    "- Performs train/test split with proper evaluation\n",
    "- Provides comprehensive performance metrics and visualizations\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, roc_auc_score, roc_curve)\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class PhysioClassifier:\n",
    "    \"\"\"Main class for physiological recording classification with customizable filtering.\"\"\"\n",
    "    \n",
    "    def __init__(self, filter_func=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Args:\n",
    "            filter_func: A function that takes a DataFrame and returns a filtered DataFrame.\n",
    "                        If None, no filtering is applied.\n",
    "        \"\"\"\n",
    "        self.filter_func = filter_func\n",
    "        self.psd_data = None\n",
    "        self.snr_data = None\n",
    "        self.ccn_data = None\n",
    "        self.combined_data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_data(self, psd_file='detailed_psd_table.csv', snr_file='detailed_snrs_table.csv', ccn_file='detailed_ccn_table.csv'):\n",
    "        \"\"\"Load PSD, SNR, and CCN data from CSV files and optionally apply filtering.\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        \n",
    "        # Load raw data\n",
    "        psd_data_raw = pd.read_csv(psd_file)\n",
    "        snr_data_raw = pd.read_csv(snr_file)\n",
    "        ccn_data_raw = pd.read_csv(ccn_file)\n",
    "        \n",
    "        print(f\"Raw PSD data shape: {psd_data_raw.shape}\")\n",
    "        print(f\"Raw SNR data shape: {snr_data_raw.shape}\")\n",
    "        print(f\"Raw CCN data shape: {ccn_data_raw.shape}\")\n",
    "        print(f\"PSD columns: {list(psd_data_raw.columns[:5])}...\")\n",
    "        print(f\"SNR columns: {list(snr_data_raw.columns)}\")\n",
    "        print(f\"CCN columns: {list(ccn_data_raw.columns)}\")\n",
    "        \n",
    "        # Display original label distribution\n",
    "        print(f\"\\nOriginal label distribution:\")\n",
    "        print(psd_data_raw['Arm Type'].value_counts())\n",
    "        \n",
    "        # Apply filtering if provided\n",
    "        if self.filter_func is not None:\n",
    "            print(\"\\nApplying custom filter...\")\n",
    "            self.psd_data = self.filter_func(psd_data_raw)\n",
    "            self.snr_data = self.filter_func(snr_data_raw)\n",
    "            self.ccn_data = self.filter_func(ccn_data_raw)\n",
    "            \n",
    "            print(f\"Filtered PSD data shape: {self.psd_data.shape}\")\n",
    "            print(f\"Filtered SNR data shape: {self.snr_data.shape}\")\n",
    "            print(f\"Filtered CCN data shape: {self.ccn_data.shape}\")\n",
    "            print(f\"Removed {len(psd_data_raw) - len(self.psd_data)} recordings\")\n",
    "            \n",
    "            print(f\"\\nFiltered label distribution:\")\n",
    "            print(self.psd_data['Arm Type'].value_counts())\n",
    "        else:\n",
    "            print(\"\\nNo filtering applied.\")\n",
    "            self.psd_data = psd_data_raw\n",
    "            self.snr_data = snr_data_raw\n",
    "            self.ccn_data = ccn_data_raw\n",
    "        \n",
    "    def combine_datasets(self):\n",
    "        \"\"\"Combine PSD, SNR, and CCN datasets on Patient, Recording, and Arm Type.\"\"\"\n",
    "        print(\"\\nCombining datasets...\")\n",
    "        \n",
    "        # First merge PSD and SNR data\n",
    "        temp_combined = pd.merge(\n",
    "            self.psd_data, \n",
    "            self.snr_data, \n",
    "            on=['Patient', 'Recording', 'Arm Type'],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Then merge with CCN data\n",
    "        self.combined_data = pd.merge(\n",
    "            temp_combined,\n",
    "            self.ccn_data,\n",
    "            on=['Patient', 'Recording', 'Arm Type'],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        print(f\"Combined data shape: {self.combined_data.shape}\")\n",
    "        print(f\"Combined label distribution:\")\n",
    "        print(self.combined_data['Arm Type'].value_counts())\n",
    "        \n",
    "        # Check for any missing values\n",
    "        missing_values = self.combined_data.isnull().sum().sum()\n",
    "        print(f\"Missing values: {missing_values}\")\n",
    "        \n",
    "        return self.combined_data\n",
    "    \n",
    "    def prepare_features_and_labels(self):\n",
    "        \"\"\"Prepare feature matrix X and target vector y.\"\"\"\n",
    "        print(\"\\nPreparing features and labels...\")\n",
    "        \n",
    "        # Identify feature columns (exclude Patient, Recording, Arm Type)\n",
    "        feature_cols = [col for col in self.combined_data.columns \n",
    "                       if col not in ['Patient', 'Recording', 'Arm Type']]\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X = self.combined_data[feature_cols].values\n",
    "        y = self.combined_data['Arm Type'].values\n",
    "        \n",
    "        print(f\"Feature matrix shape: {X.shape}\")\n",
    "        print(f\"Number of features: {len(feature_cols)}\")\n",
    "        print(f\"Feature types: PSD ({len([c for c in feature_cols if 'PSD' in c])}), \"\n",
    "              f\"SNR ({len([c for c in feature_cols if 'SNR' in c])}), \"\n",
    "              f\"CCN ({len([c for c in feature_cols if 'CCN' in c])})\")\n",
    "        \n",
    "        return X, y, feature_cols\n",
    "    \n",
    "    def split_and_scale_data(self, X, y, test_size=0.15, random_state=42):\n",
    "        \"\"\"Split data into train/test sets and apply scaling.\"\"\"\n",
    "        print(f\"\\nSplitting data (test_size={test_size})...\")\n",
    "        \n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale the features\n",
    "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        print(f\"Training set: {self.X_train.shape[0]} samples\")\n",
    "        print(f\"Test set: {self.X_test.shape[0]} samples\")\n",
    "        print(f\"Training label distribution:\")\n",
    "        unique, counts = np.unique(self.y_train, return_counts=True)\n",
    "        for label, count in zip(unique, counts):\n",
    "            print(f\"  {label}: {count} ({count/len(self.y_train)*100:.1f}%)\")\n",
    "            \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Initialize different classifier models.\"\"\"\n",
    "        print(\"\\nInitializing models...\")\n",
    "        \n",
    "        self.models = {\n",
    "            'Decision Tree': DecisionTreeClassifier(\n",
    "                random_state=42,\n",
    "                max_depth=5,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2\n",
    "            ),\n",
    "            'Random Forest': RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                random_state=42,\n",
    "                max_depth=8,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                max_features='sqrt'\n",
    "            ),\n",
    "            'SVM (RBF)': SVC(\n",
    "                kernel='rbf',\n",
    "                random_state=42,\n",
    "                probability=True,\n",
    "                C=10.0,\n",
    "                gamma='scale'\n",
    "            ),\n",
    "            'SVM (Linear)': SVC(\n",
    "                kernel='linear',\n",
    "                random_state=42,\n",
    "                probability=True,\n",
    "                C=1.0\n",
    "            ),\n",
    "            'Logistic Regression': LogisticRegression(\n",
    "                random_state=42,\n",
    "                max_iter=2000,\n",
    "                C=0.1,\n",
    "                class_weight='balanced'\n",
    "            ),\n",
    "            'K-Nearest Neighbors': KNeighborsClassifier(\n",
    "                n_neighbors=7,\n",
    "                weights='distance',\n",
    "                metric='minkowski',\n",
    "                p=2\n",
    "            ),\n",
    "            'Naive Bayes': GaussianNB(var_smoothing=1e-9)\n",
    "        }\n",
    "        \n",
    "        print(f\"Initialized {len(self.models)} models: {list(self.models.keys())}\")\n",
    "    \n",
    "    def train_and_evaluate_models(self):\n",
    "        \"\"\"Train all models and evaluate their performance.\"\"\"\n",
    "        print(\"\\nTraining and evaluating models...\")\n",
    "        \n",
    "        self.results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            y_pred_proba = model.predict_proba(self.X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(self.y_test, y_pred)\n",
    "            precision = precision_score(self.y_test, y_pred, pos_label='Healthy')\n",
    "            recall = recall_score(self.y_test, y_pred, pos_label='Healthy')\n",
    "            f1 = f1_score(self.y_test, y_pred, pos_label='Healthy')\n",
    "            \n",
    "            # ROC AUC (if probabilities available)\n",
    "            roc_auc = None\n",
    "            if y_pred_proba is not None:\n",
    "                # Convert labels to binary for ROC calculation\n",
    "                y_test_binary = (self.y_test == 'Healthy').astype(int)\n",
    "                roc_auc = roc_auc_score(y_test_binary, y_pred_proba)\n",
    "            \n",
    "            # Cross-validation score\n",
    "            cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='accuracy')\n",
    "            \n",
    "            # Store results\n",
    "            self.results[name] = {\n",
    "                'model': model,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'roc_auc': roc_auc,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'y_pred': y_pred,\n",
    "                'y_pred_proba': y_pred_proba,\n",
    "                'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "            }\n",
    "            \n",
    "            print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "            print(f\"  Precision: {precision:.3f}\")\n",
    "            print(f\"  Recall: {recall:.3f}\")\n",
    "            print(f\"  F1-score: {f1:.3f}\")\n",
    "            if roc_auc:\n",
    "                print(f\"  ROC AUC: {roc_auc:.3f}\")\n",
    "            print(f\"  CV Score: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    def print_results_summary(self):\n",
    "        \"\"\"Print a summary table of all model results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_data = []\n",
    "        for name, result in self.results.items():\n",
    "            summary_data.append({\n",
    "                'Model': name,\n",
    "                'Accuracy': f\"{result['accuracy']:.3f}\",\n",
    "                'Precision': f\"{result['precision']:.3f}\",\n",
    "                'Recall': f\"{result['recall']:.3f}\",\n",
    "                'F1-Score': f\"{result['f1_score']:.3f}\",\n",
    "                'ROC AUC': f\"{result['roc_auc']:.3f}\" if result['roc_auc'] else \"N/A\",\n",
    "                'CV Score': f\"{result['cv_mean']:.3f} ± {result['cv_std']:.3f}\"\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        print(summary_df.to_string(index=False))\n",
    "        \n",
    "        # Find best model\n",
    "        best_model_name = max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])\n",
    "        print(f\"\\nBest performing model: {best_model_name}\")\n",
    "        print(f\"Best accuracy: {self.results[best_model_name]['accuracy']:.3f}\")\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Create visualization plots for model comparison.\"\"\"\n",
    "        print(\"\\nGenerating plots...\")\n",
    "        \n",
    "        # Set up the plotting\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        model_names = list(self.results.keys())\n",
    "        accuracies = [self.results[name]['accuracy'] for name in model_names]\n",
    "        f1_scores = [self.results[name]['f1_score'] for name in model_names]\n",
    "        cv_means = [self.results[name]['cv_mean'] for name in model_names]\n",
    "        cv_stds = [self.results[name]['cv_std'] for name in model_names]\n",
    "        \n",
    "        # 1. Accuracy comparison\n",
    "        axes[0, 0].bar(model_names, accuracies, color='skyblue', alpha=0.7)\n",
    "        axes[0, 0].set_title('Test Accuracy by Model')\n",
    "        axes[0, 0].set_ylabel('Accuracy')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. F1-Score comparison\n",
    "        axes[0, 1].bar(model_names, f1_scores, color='lightgreen', alpha=0.7)\n",
    "        axes[0, 1].set_title('F1-Score by Model')\n",
    "        axes[0, 1].set_ylabel('F1-Score')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Cross-validation scores with error bars\n",
    "        axes[1, 0].bar(model_names, cv_means, yerr=cv_stds, capsize=5, \n",
    "                      color='orange', alpha=0.7)\n",
    "        axes[1, 0].set_title('Cross-Validation Accuracy (5-fold)')\n",
    "        axes[1, 0].set_ylabel('CV Accuracy')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. ROC curves for models with probability predictions\n",
    "        axes[1, 1].set_title('ROC Curves')\n",
    "        axes[1, 1].set_xlabel('False Positive Rate')\n",
    "        axes[1, 1].set_ylabel('True Positive Rate')\n",
    "        \n",
    "        # Convert labels to binary for ROC\n",
    "        y_test_binary = (self.y_test == 'Healthy').astype(int)\n",
    "        \n",
    "        for name, result in self.results.items():\n",
    "            if result['y_pred_proba'] is not None:\n",
    "                fpr, tpr, _ = roc_curve(y_test_binary, result['y_pred_proba'])\n",
    "                axes[1, 1].plot(fpr, tpr, label=f\"{name} (AUC={result['roc_auc']:.3f})\")\n",
    "        \n",
    "        axes[1, 1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrices(self):\n",
    "        \"\"\"Plot confusion matrices for all models.\"\"\"\n",
    "        n_models = len(self.results)\n",
    "        cols = 3\n",
    "        rows = (n_models + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "        fig.suptitle('Confusion Matrices', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for idx, (name, result) in enumerate(self.results.items()):\n",
    "            row, col = idx // cols, idx % cols\n",
    "            ax = axes[row, col] if rows > 1 else axes[col]\n",
    "            \n",
    "            cm = result['confusion_matrix']\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                       xticklabels=['Healthy', 'Impaired'], \n",
    "                       yticklabels=['Healthy', 'Impaired'])\n",
    "            ax.set_title(f'{name}\\nAccuracy: {result[\"accuracy\"]:.3f}')\n",
    "            ax.set_xlabel('Predicted')\n",
    "            ax.set_ylabel('Actual')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for idx in range(n_models, rows * cols):\n",
    "            row, col = idx // cols, idx % cols\n",
    "            ax = axes[row, col] if rows > 1 else axes[col]\n",
    "            ax.set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_feature_importance(self):\n",
    "        \"\"\"Analyze feature importance for tree-based models.\"\"\"\n",
    "        print(\"\\nAnalyzing feature importance...\")\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_cols = [col for col in self.combined_data.columns \n",
    "                       if col not in ['Patient', 'Recording', 'Arm Type']]\n",
    "        \n",
    "        # Analyze Random Forest feature importance\n",
    "        if 'Random Forest' in self.results:\n",
    "            rf_model = self.results['Random Forest']['model']\n",
    "            feature_importance = rf_model.feature_importances_\n",
    "            \n",
    "            # Create feature importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_cols,\n",
    "                'importance': feature_importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "            print(importance_df.head(10).to_string(index=False))\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_features = importance_df.head(15)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run the complete classification analysis pipeline.\"\"\"\n",
    "        print(\"Starting Physiological Recording Classification Analysis\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load and combine data\n",
    "        self.load_data()\n",
    "        self.combine_datasets()\n",
    "        \n",
    "        # Prepare features and labels\n",
    "        X, y, feature_cols = self.prepare_features_and_labels()\n",
    "        \n",
    "        # Split and scale data\n",
    "        self.split_and_scale_data(X, y)\n",
    "        \n",
    "        # Initialize and train models\n",
    "        self.initialize_models()\n",
    "        self.train_and_evaluate_models()\n",
    "        \n",
    "        # Display results\n",
    "        self.print_results_summary()\n",
    "        \n",
    "        # Generate visualizations\n",
    "        self.plot_results()\n",
    "        self.plot_confusion_matrices()\n",
    "        self.analyze_feature_importance()\n",
    "        \n",
    "        print(\"\\nAnalysis complete! Check the generated plots:\")\n",
    "        print(\"- model_comparison.png: Overall model performance\")\n",
    "        print(\"- confusion_matrices.png: Detailed confusion matrices\")\n",
    "        print(\"- feature_importance.png: Most important features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FILTER FUNCTION EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "def no_filter(df):\n",
    "    \"\"\"No filtering - return all data.\"\"\"\n",
    "    return df\n",
    "\n",
    "def fma_filter_healthy_only_perfect(df):\n",
    "    \"\"\"\n",
    "    Filter to exclude healthy recordings with FMA score ≠ 2.\n",
    "    Keeps all impaired recordings regardless of FMA score.\n",
    "    \"\"\"\n",
    "    # Load FMA data\n",
    "    try:\n",
    "        fma_data = pd.read_csv('detailed_fma_table.csv')\n",
    "        print(f\"Loaded FMA data for filtering: {fma_data.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: FMA data not found. Returning unfiltered data.\")\n",
    "        return df\n",
    "    \n",
    "    # Merge with FMA data\n",
    "    df_with_fma = pd.merge(df, fma_data, on=['Patient', 'Recording', 'Arm Type'], how='left')\n",
    "    \n",
    "    # Apply filter: Keep all impaired, only healthy with FMA = 2\n",
    "    filter_mask = (\n",
    "        (df_with_fma['Arm Type'] == 'Impaired') |  # Keep all impaired\n",
    "        ((df_with_fma['Arm Type'] == 'Healthy') & (df_with_fma['Average FMA Score'] == 2.0))  # Only perfect healthy\n",
    "    )\n",
    "    \n",
    "    filtered_df = df_with_fma[filter_mask].drop('Average FMA Score', axis=1)\n",
    "    \n",
    "    # Report filtering results\n",
    "    excluded = len(df) - len(filtered_df)\n",
    "    print(f\"FMA Filter applied: Excluded {excluded} recordings\")\n",
    "    print(f\"Remaining: {filtered_df['Arm Type'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def fma_filter_healthy_and_impaired_only_perfect(df):\n",
    "    \"\"\"\n",
    "    Filter to exclude healthy recordings with FMA score ≠ 2.\n",
    "    It also excludes impaired recordings with FMA score = 2.\n",
    "    Keeps all impaired recordings regardless of FMA score.\n",
    "    \"\"\"\n",
    "    # Load FMA data\n",
    "    try:\n",
    "        fma_data = pd.read_csv('detailed_fma_table.csv')\n",
    "        print(f\"Loaded FMA data for filtering: {fma_data.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: FMA data not found. Returning unfiltered data.\")\n",
    "        return df\n",
    "    \n",
    "    # Merge with FMA data\n",
    "    df_with_fma = pd.merge(df, fma_data, on=['Patient', 'Recording', 'Arm Type'], how='left')\n",
    "    \n",
    "    # Apply filter: Keep all impaired, only healthy with FMA = 2\n",
    "    filter_mask = (\n",
    "        (df_with_fma['Arm Type'] == 'Impaired') & (df_with_fma['Average FMA Score'] > 1.0) |  # Keep all impaired\n",
    "        ((df_with_fma['Arm Type'] == 'Healthy') & (df_with_fma['Average FMA Score'] == 2.0))  # Only perfect healthy\n",
    "    )\n",
    "    \n",
    "    filtered_df = df_with_fma[filter_mask].drop('Average FMA Score', axis=1)\n",
    "    \n",
    "    # Report filtering results\n",
    "    excluded = len(df) - len(filtered_df)\n",
    "    print(f\"FMA Filter applied: Excluded {excluded} recordings\")\n",
    "    print(f\"Remaining: {filtered_df['Arm Type'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def fma_filter_range(min_fma=1.5, max_fma=2.0):\n",
    "    \"\"\"\n",
    "    Create a filter function for recordings by FMA score range.\n",
    "    \n",
    "    Args:\n",
    "        min_fma: Minimum FMA score to include\n",
    "        max_fma: Maximum FMA score to include\n",
    "        \n",
    "    Returns:\n",
    "        A filter function that can be used with PhysioClassifier\n",
    "    \"\"\"\n",
    "    def filter_func(df):\n",
    "        try:\n",
    "            fma_data = pd.read_csv('detailed_fma_table.csv')\n",
    "        except FileNotFoundError:\n",
    "            print(\"Warning: FMA data not found. Returning unfiltered data.\")\n",
    "            return df\n",
    "        \n",
    "        # Merge with FMA data\n",
    "        df_with_fma = pd.merge(df, fma_data, on=['Patient', 'Recording', 'Arm Type'], how='left')\n",
    "        \n",
    "        # Apply FMA range filter\n",
    "        filter_mask = (\n",
    "            (df_with_fma['Average FMA Score'] >= min_fma) & \n",
    "            (df_with_fma['Average FMA Score'] <= max_fma)\n",
    "        )\n",
    "        \n",
    "        filtered_df = df_with_fma[filter_mask].drop('Average FMA Score', axis=1)\n",
    "        \n",
    "        excluded = len(df) - len(filtered_df)\n",
    "        print(f\"FMA Range Filter ({min_fma}-{max_fma}): Excluded {excluded} recordings\")\n",
    "        print(f\"Remaining: {filtered_df['Arm Type'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    return filter_func\n",
    "\n",
    "def patient_filter(patient_list):\n",
    "    \"\"\"\n",
    "    Create a filter function to include only specific patients.\n",
    "    \n",
    "    Args:\n",
    "        patient_list: List of patient numbers to include\n",
    "        \n",
    "    Returns:\n",
    "        A filter function that can be used with PhysioClassifier\n",
    "    \"\"\"\n",
    "    def filter_func(df):\n",
    "        filter_mask = df['Patient'].isin(patient_list)\n",
    "        filtered_df = df[filter_mask]\n",
    "        \n",
    "        excluded = len(df) - len(filtered_df)\n",
    "        print(f\"Patient Filter (patients {patient_list}): Excluded {excluded} recordings\")\n",
    "        print(f\"Remaining: {filtered_df['Arm Type'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    return filter_func\n",
    "\n",
    "def snr_threshold_filter(min_snr=5.0):\n",
    "    \"\"\"\n",
    "    Create a filter function for recordings by minimum SNR threshold.\n",
    "    \n",
    "    Args:\n",
    "        min_snr: Minimum SNR Mean (dB) to include\n",
    "        \n",
    "    Returns:\n",
    "        A filter function that can be used with PhysioClassifier\n",
    "    \"\"\"\n",
    "    def filter_func(df):\n",
    "        if 'SNR Mean (dB)' not in df.columns:\n",
    "            print(\"Warning: SNR data not found in DataFrame. Returning unfiltered data.\")\n",
    "            return df\n",
    "        \n",
    "        filter_mask = df['SNR Mean (dB)'] >= min_snr\n",
    "        filtered_df = df[filter_mask]\n",
    "        \n",
    "        excluded = len(df) - len(filtered_df)\n",
    "        print(f\"SNR Filter (≥{min_snr} dB): Excluded {excluded} recordings\")\n",
    "        print(f\"Remaining: {filtered_df['Arm Type'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    return filter_func\n",
    "\n",
    "def balanced_dataset_filter(random_state=42):\n",
    "    \"\"\"\n",
    "    Create a filter function for balanced dataset with equal numbers of healthy and impaired recordings.\n",
    "    \n",
    "    Args:\n",
    "        random_state: Random seed for sampling\n",
    "        \n",
    "    Returns:\n",
    "        A filter function that can be used with PhysioClassifier\n",
    "    \"\"\"\n",
    "    def filter_func(df):\n",
    "        healthy_data = df[df['Arm Type'] == 'Healthy']\n",
    "        impaired_data = df[df['Arm Type'] == 'Impaired']\n",
    "        \n",
    "        min_samples = min(len(healthy_data), len(impaired_data))\n",
    "        \n",
    "        balanced_df = pd.concat([\n",
    "            healthy_data.sample(n=min_samples, random_state=random_state),\n",
    "            impaired_data.sample(n=min_samples, random_state=random_state)\n",
    "        ])\n",
    "        \n",
    "        excluded = len(df) - len(balanced_df)\n",
    "        print(f\"Balanced Filter: Excluded {excluded} recordings\")\n",
    "        print(f\"Remaining: {balanced_df['Arm Type'].value_counts().to_dict()}\")\n",
    "        \n",
    "        return balanced_df\n",
    "    \n",
    "    return filter_func\n",
    "\n",
    "# Lambda examples for quick filters\n",
    "exclude_patient_1 = lambda df: df[df['Patient'] != 1]\n",
    "only_first_10_patients = lambda df: df[df['Patient'] <= 10]\n",
    "high_snr_only = lambda df: df[df['SNR Mean (dB)'] > 10] if 'SNR Mean (dB)' in df.columns else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deab572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis WITH FMA filtering (exclude healthy recordings with FMA ≠ 2)\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING ANALYSIS WITH FMA FILTERING\")\n",
    "print(\"Filter: Exclude healthy recordings with FMA score ≠ 2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "classifier_fma_filtered = PhysioClassifier(filter_func=fma_filter_healthy_only_perfect)\n",
    "classifier_fma_filtered.run_complete_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986295b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis WITHOUT filtering (original approach)\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING ANALYSIS WITHOUT FILTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "classifier_unfiltered = PhysioClassifier()\n",
    "classifier_unfiltered.run_complete_analysis()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
