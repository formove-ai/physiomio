{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3eb39af5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Physiological Recording Classification: Healthy vs Impaired\n",
                "# Simple Random Forest classifier with patient-based splits\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
                "\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random state for reproducibility\n",
                "RANDOM_STATE = 123\n",
                "np.random.seed(RANDOM_STATE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2fa68c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the normalized data tables\n",
                "print(\"Loading normalized data tables...\")\n",
                "\n",
                "# Load PSD and SNR normalized data\n",
                "psd_data = pd.read_csv('detailed_psd_normalized_table.csv')\n",
                "snr_data = pd.read_csv('detailed_snrs_normalized_table.csv')\n",
                "\n",
                "print(f\"PSD data shape: {psd_data.shape}\")\n",
                "print(f\"SNR data shape: {snr_data.shape}\")\n",
                "print(f\"\\nLabel distribution in PSD data:\")\n",
                "print(psd_data['Arm Type'].value_counts())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1a16224a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine datasets and prepare features\n",
                "print(\"Combining datasets...\")\n",
                "\n",
                "# Merge PSD and SNR data on Patient, Recording, and Arm Type\n",
                "combined_data = pd.merge(\n",
                " psd_data, \n",
                " snr_data, \n",
                " on=['Patient', 'Recording', 'Arm Type'],\n",
                " how='inner'\n",
                ")\n",
                "\n",
                "print(f\"Combined data shape: {combined_data.shape}\")\n",
                "print(f\"Combined label distribution:\")\n",
                "print(combined_data['Arm Type'].value_counts())\n",
                "\n",
                "# Prepare features and labels\n",
                "feature_cols = [col for col in combined_data.columns if col not in ['Patient', 'Recording', 'Arm Type']]\n",
                "\n",
                "X = combined_data[feature_cols].values\n",
                "y = combined_data['Arm Type'].values\n",
                "\n",
                "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
                "print(f\"Number of features: {len(feature_cols)}\")\n",
                "print(f\"PSD features: {len([c for c in feature_cols if 'PSD' in c])}\")\n",
                "print(f\"SNR features: {len([c for c in feature_cols if 'SNR' in c])}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e2f2c16",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Patient-based train/test split function\n",
                "def patient_based_split(combined_data, X, y, test_size=0.15, random_state=RANDOM_STATE):\n",
                "    \"\"\"\n",
                "    Split data ensuring no patient appears in both train and test sets.\n",
                "    \"\"\"\n",
                "    np.random.seed(random_state)\n",
                "\n",
                "    # Get patient recording counts\n",
                "    patient_data = combined_data.groupby('Patient').agg({\n",
                "    'Recording': 'count',\n",
                "    'Arm Type': lambda x: list(x)\n",
                "    }).rename(columns={'Recording': 'num_recordings', 'Arm Type': 'arm_types'})\n",
                "\n",
                "    # Calculate target test size\n",
                "    total_recordings = len(combined_data)\n",
                "    target_test_recordings = int(total_recordings * test_size)\n",
                "\n",
                "    # Select patients for test set using greedy approach\n",
                "    patients_by_size = patient_data.sort_values('num_recordings', ascending=False)\n",
                "    remaining_patients = list(patients_by_size.index)\n",
                "\n",
                "    best_test_patients = []\n",
                "    best_diff = float('inf')\n",
                "\n",
                "    # Try multiple random combinations to find best split\n",
                "    for _ in range(100):  # Multiple random attempts\n",
                "        np.random.shuffle(remaining_patients)\n",
                "        current_test_patients = []\n",
                "        current_count = 0\n",
                "        \n",
                "        for patient in remaining_patients:\n",
                "            patient_recordings = patient_data.loc[patient, 'num_recordings']\n",
                "            if current_count + patient_recordings <= target_test_recordings + 5:  # Allow small overshoot\n",
                "                current_test_patients.append(patient)\n",
                "                current_count += patient_recordings\n",
                "                \n",
                "                # If we're close enough, stop\n",
                "                if abs(current_count - target_test_recordings) <= abs(best_diff):\n",
                "                    if abs(current_count - target_test_recordings) < abs(best_diff):\n",
                "                        best_test_patients = current_test_patients.copy()\n",
                "                        best_diff = current_count - target_test_recordings\n",
                "\n",
                "    test_patients = best_test_patients\n",
                "    test_recordings_count = sum(patient_data.loc[p, 'num_recordings'] for p in test_patients)\n",
                "\n",
                "    print(f\"Selected {len(test_patients)} patients for test set\")\n",
                "    print(f\"Test recordings: {test_recordings_count} ({test_recordings_count/total_recordings*100:.1f}%)\")\n",
                "\n",
                "    # Create train/test masks\n",
                "    test_mask = combined_data['Patient'].isin(test_patients)\n",
                "    train_mask = ~test_mask\n",
                "\n",
                "    # Split the data\n",
                "    X_train = X[train_mask]\n",
                "    X_test = X[test_mask]\n",
                "    y_train = y[train_mask]\n",
                "    y_test = y[test_mask]\n",
                "\n",
                "    # Verify no patient overlap\n",
                "    train_patients = set(combined_data[train_mask]['Patient'].unique())\n",
                "    test_patients_actual = set(combined_data[test_mask]['Patient'].unique())\n",
                "    overlap = train_patients.intersection(test_patients_actual)\n",
                "\n",
                "    # Return patient IDs aligned to X_test row order\n",
                "    test_patient_ids = combined_data.loc[test_mask, \"Patient\"].to_numpy()\n",
                "\n",
                "    if overlap:\n",
                "        print(f\"WARNING: Patient overlap detected: {overlap}\")\n",
                "    else:\n",
                "        print(\"✓ No patient overlap between train and test sets\")\n",
                "\n",
                "    return X_train, X_test, y_train, y_test, test_patient_ids\n",
                "\n",
                "# Perform the split\n",
                "print(\"Performing patient-based train/test split...\")\n",
                "X_train, X_test, y_train, y_test, test_patient_ids = patient_based_split(combined_data, X, y)\n",
                "\n",
                "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]} samples\")\n",
                "print(f\"Training label distribution:\")\n",
                "unique, counts = np.unique(y_train, return_counts=True)\n",
                "for label, count in zip(unique, counts):\n",
                " print(f\" {label}: {count} ({count/len(y_train)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9a5a7230",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model creation function\n",
                "def create_rf_model():\n",
                "    \"\"\"Create a RandomForestClassifier with consistent parameters.\"\"\"\n",
                "    return RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    random_state=RANDOM_STATE,\n",
                "    max_depth=10,\n",
                "    min_samples_split=5,\n",
                "    min_samples_leaf=2,\n",
                "    max_features='sqrt',\n",
                "    class_weight='balanced'\n",
                "    )\n",
                "\n",
                "# Scale features and train Random Forest\n",
                "print(\"Scaling features and training Random Forest...\")\n",
                "\n",
                "# Scale the features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Initialize and train Random Forest\n",
                "rf_model = create_rf_model()\n",
                "\n",
                "# Train the model\n",
                "rf_model.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = rf_model.predict(X_test_scaled)\n",
                "\n",
                "# Calculate test accuracy\n",
                "test_accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"\\nTest Accuracy: {test_accuracy:.3f}\")\n",
                "\n",
                "# Print detailed classification report\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred, digits=3))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "84e09057",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-validation with patient-based splits\n",
                "def patient_based_cv_split(combined_data, n_splits=5, random_state=RANDOM_STATE):\n",
                "    \"\"\"\n",
                "    Create cross-validation splits ensuring no patient appears in multiple folds.\n",
                "    \"\"\"\n",
                "    np.random.seed(random_state)\n",
                "\n",
                "    # Get unique patients\n",
                "    unique_patients = combined_data['Patient'].unique()\n",
                "    np.random.shuffle(unique_patients)\n",
                "\n",
                "    # Split patients into folds\n",
                "    fold_size = len(unique_patients) // n_splits\n",
                "    patient_folds = []\n",
                "\n",
                "    for i in range(n_splits):\n",
                "        start_idx = i * fold_size\n",
                "        if i == n_splits - 1:  # Last fold gets remaining patients\n",
                "            end_idx = len(unique_patients)\n",
                "        else:\n",
                "            end_idx = (i + 1) * fold_size\n",
                "        patient_folds.append(unique_patients[start_idx:end_idx])\n",
                "\n",
                "    # Create train/validation indices for each fold\n",
                "    cv_splits = []\n",
                "    for i in range(n_splits):\n",
                "        val_patients = patient_folds[i]\n",
                "        val_mask = combined_data['Patient'].isin(val_patients)\n",
                "        train_mask = ~val_mask\n",
                "\n",
                "        train_indices = combined_data[train_mask].index.values\n",
                "        val_indices = combined_data[val_mask].index.values\n",
                "\n",
                "        cv_splits.append((train_indices, val_indices))\n",
                "\n",
                "    return cv_splits\n",
                "\n",
                "print(\"Performing cross-validation with patient-based splits...\")\n",
                "\n",
                "# ensure balanced_accuracy_score is available at cell level\n",
                "from sklearn.metrics import balanced_accuracy_score\n",
                "\n",
                "# Create patient-based CV splits\n",
                "cv_splits = patient_based_cv_split(combined_data, n_splits=5)\n",
                "\n",
                "# Perform cross-validation\n",
                "cv_scores = []\n",
                "cv_balanced_scores = []\n",
                "for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
                "    # Get train and validation data for this fold\n",
                "    X_fold_train = X[train_idx]\n",
                "    X_fold_val = X[val_idx]\n",
                "    y_fold_train = y[train_idx]\n",
                "    y_fold_val = y[val_idx]\n",
                "\n",
                "    # Scale features\n",
                "    fold_scaler = StandardScaler()\n",
                "    X_fold_train_scaled = fold_scaler.fit_transform(X_fold_train)\n",
                "    X_fold_val_scaled = fold_scaler.transform(X_fold_val)\n",
                "\n",
                "    # Train model\n",
                "    fold_model = create_rf_model()\n",
                "    fold_model.fit(X_fold_train_scaled, y_fold_train)\n",
                "\n",
                "    # Evaluate\n",
                "    fold_pred = fold_model.predict(X_fold_val_scaled)\n",
                "    fold_accuracy = accuracy_score(y_fold_val, fold_pred)\n",
                "    fold_balanced = balanced_accuracy_score(y_fold_val, fold_pred)\n",
                "    cv_scores.append(fold_accuracy)\n",
                "    cv_balanced_scores.append(fold_balanced)\n",
                "\n",
                "    print(f\"Fold {fold + 1}: Accuracy={fold_accuracy:.3f}, BalancedAcc={fold_balanced:.3f}\")\n",
                "\n",
                "cv_mean = np.mean(cv_scores)\n",
                "cv_std = np.std(cv_scores)\n",
                "cv_mean_bal = np.mean(cv_balanced_scores)\n",
                "cv_std_bal = np.std(cv_balanced_scores)\n",
                "\n",
                "print(f\"\\nCross-Validation Results:\")\n",
                "print(f\"Mean CV Accuracy: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
                "print(f\"Mean CV BalancedAccuracy: {cv_mean_bal:.3f} ± {cv_std_bal:.3f}\")\n",
                "print(f\"Individual fold scores (Accuracy): {[f'{score:.3f}' for score in cv_scores]}\")\n",
                "print(f\"Individual fold scores (BalancedAcc): {[f'{score:.3f}' for score in cv_balanced_scores]}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "008486c6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot confusion matrix for test set\n",
                "print(\"Plotting confusion matrix...\")\n",
                "\n",
                "# Create confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "# Plot confusion matrix\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                " xticklabels=['Healthy', 'Impaired'], \n",
                " yticklabels=['Healthy', 'Impaired'])\n",
                "plt.title(f'Confusion Matrix - Random Forest\\nTest Accuracy: {test_accuracy:.3f}')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print summary\n",
                "print(f\"\\nSUMMARY:\")\n",
                "print(f\"========\")\n",
                "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
                "print(f\"Cross-Validation Accuracy: {cv_mean:.3f} ± {cv_std:.3f}\")\n",
                "print(f\"Training samples: {len(y_train)}\")\n",
                "print(f\"Test samples: {len(y_test)}\")\n",
                "print(f\"Features used: {X.shape[1]} (PSD + SNR normalized features)\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ea0efa01",
            "metadata": {},
            "outputs": [],
            "source": [
                "# add in SUMMARY cell (after Test Accuracy line)\n",
                "POS_LABEL = \"Impaired\"\n",
                "\n",
                "def _pos_index(model, pos_label=POS_LABEL):\n",
                " return list(model.classes_).index(pos_label)\n",
                "\n",
                "rf_proba_test = rf_model.predict_proba(X_test_scaled)[\n",
                " :, _pos_index(rf_model, \"Impaired\")\n",
                "]\n",
                "rf_pred_test = y_pred # already computed earlier\n",
                "from sklearn.metrics import (\n",
                " balanced_accuracy_score,\n",
                " f1_score,\n",
                " roc_auc_score,\n",
                " average_precision_score,\n",
                ")\n",
                "import pandas as pd\n",
                "y_bin = (pd.Series(y_test) == \"Impaired\").astype(int).to_numpy()\n",
                "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, rf_pred_test):.3f}\")\n",
                "print(f\"Macro F1: {f1_score(y_test, rf_pred_test, average='macro'):.3f}\")\n",
                "print(f\"AUROC: {roc_auc_score(y_bin, rf_proba_test):.3f}\")\n",
                "print(f\"AUPRC: {average_precision_score(y_bin, rf_proba_test):.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aa87bfd5",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.metrics import (\n",
                " accuracy_score,\n",
                " balanced_accuracy_score,\n",
                " f1_score,\n",
                " roc_auc_score,\n",
                " average_precision_score,\n",
                ")\n",
                "\n",
                "rng = np.random.default_rng(RANDOM_STATE)\n",
                "\n",
                "# Build a dataframe to keep things aligned\n",
                "POS_LABEL = \"Impaired\"\n",
                "y_true = pd.Series(y_test)\n",
                "y_hat = pd.Series(rf_pred) if \"rf_pred\" in globals() else pd.Series(y_pred)\n",
                "y_prob = pd.Series(rf_proba_test)\n",
                "pid = pd.Series(test_patient_ids)\n",
                "\n",
                "test_df = pd.DataFrame(\n",
                "    {\n",
                "    \"y_true\": y_true.values,\n",
                "    \"y_hat\": y_hat.values,\n",
                "    \"y_prob\": y_prob.values,\n",
                "    \"pid\": pid.values,\n",
                "    }\n",
                ")\n",
                "\n",
                "\n",
                "# Helper: compute all metrics on a slice of test_df\n",
                "def _compute_metrics(df):\n",
                "    # binary array for threshold-free metrics\n",
                "    y_bin = (df[\"y_true\"] == POS_LABEL).astype(int).to_numpy()\n",
                "\n",
                "    out = {}\n",
                "    out[\"Accuracy\"] = accuracy_score(df[\"y_true\"], df[\"y_hat\"])\n",
                "    out[\"BalancedAccuracy\"] = balanced_accuracy_score(df[\"y_true\"], df[\"y_hat\"])\n",
                "    out[\"MacroF1\"] = f1_score(df[\"y_true\"], df[\"y_hat\"], average=\"macro\")\n",
                "\n",
                "    # AUROC / AUPRC may be undefined if resample has one class\n",
                "    try:\n",
                "        out[\"AUROC\"] = roc_auc_score(y_bin, df[\"y_prob\"].to_numpy())\n",
                "    except Exception:\n",
                "        out[\"AUROC\"] = np.nan\n",
                "    try:\n",
                "        out[\"AUPRC\"] = average_precision_score(y_bin, df[\"y_prob\"].to_numpy())\n",
                "    except Exception:\n",
                "        out[\"AUPRC\"] = np.nan\n",
                "    return out\n",
                "\n",
                "\n",
                "# Point estimates on the full test set (for reference)\n",
                "pt = _compute_metrics(test_df)\n",
                "\n",
                "\n",
                "# Clustered bootstrap over patients\n",
                "def bootstrap_patient_ci(test_df, n_boot=5000, alpha=0.05, seed=RANDOM_STATE):\n",
                "    rng = np.random.default_rng(seed)\n",
                "    pids = test_df[\"pid\"].unique()\n",
                "    B = {k: [] for k in [\"Accuracy\", \"BalancedAccuracy\", \"MacroF1\", \"AUROC\", \"AUPRC\"]}\n",
                "\n",
                "    for _ in range(n_boot):\n",
                "    # sample patients with replacement\n",
                "        boot_pids = rng.choice(pids, size=len(pids), replace=True)\n",
                "        boot_df = pd.concat([test_df[test_df[\"pid\"] == p] for p in boot_pids], axis=0)\n",
                "        m = _compute_metrics(boot_df)\n",
                "        for k in B:\n",
                "            B[k].append(m[k])\n",
                "\n",
                "        # Percentile CIs (drop NaNs for AUROC/AUPRC)\n",
                "        ci = {}\n",
                "        for k, vals in B.items():\n",
                "            arr = np.asarray(vals, dtype=float)\n",
                "            arr = arr[~np.isnan(arr)]\n",
                "            lo, hi = np.quantile(arr, [alpha / 2, 1 - alpha / 2])\n",
                "            ci[k] = (lo, hi, arr.size)\n",
                "    return ci, pt\n",
                "\n",
                "\n",
                "ci, pt = bootstrap_patient_ci(test_df, n_boot=5000, alpha=0.05, seed=RANDOM_STATE)\n",
                "\n",
                "\n",
                "# Pretty print\n",
                "def _fmt(v):\n",
                " return f\"{v:.3f}\"\n",
                "\n",
                "\n",
                "print(\"Patient-clustered 95% bootstrap CIs (percentile):\")\n",
                "for k in [\"Accuracy\", \"BalancedAccuracy\", \"MacroF1\", \"AUROC\", \"AUPRC\"]:\n",
                "    lo, hi, n_used = ci[k]\n",
                "    print(f\" {k:17s} { _fmt(pt[k]) } [{_fmt(lo)}, {_fmt(hi)}] (boot n={n_used})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a3582268",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === LOPO (leave-one-patient-out) — recording-level evaluation (arms) ===\n",
                "# Assumes: combined_data, feature_cols, create_rf_model, RANDOM_STATE exist.\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import (\n",
                " accuracy_score,\n",
                " balanced_accuracy_score,\n",
                " f1_score,\n",
                " classification_report,\n",
                ")\n",
                "\n",
                "POS_LABEL = \"Impaired\" # arm label considered positive\n",
                "\n",
                "X_all = combined_data[feature_cols].to_numpy()\n",
                "y_all = combined_data[\"Arm Type\"].to_numpy() # Healthy / Impaired (arm-level)\n",
                "pids = combined_data[\"Patient\"].to_numpy()\n",
                "\n",
                "unique_pids = np.array(sorted(pd.unique(pids)))\n",
                "\n",
                "# Storage for pooled (micro) metrics across all LOPO folds\n",
                "pooled_y_true = []\n",
                "pooled_y_pred = []\n",
                "\n",
                "# Per-patient metrics table\n",
                "rows = []\n",
                "\n",
                "for pid in unique_pids:\n",
                "    test_mask = pids == pid\n",
                "    train_mask = ~test_mask\n",
                "\n",
                "    # Fit scaler on train-patients only (no leakage)\n",
                "    scaler = StandardScaler()\n",
                "    X_tr = scaler.fit_transform(X_all[train_mask])\n",
                "    X_te = scaler.transform(X_all[test_mask])\n",
                "    y_tr = y_all[train_mask]\n",
                "    y_te = y_all[test_mask]\n",
                "\n",
                "    # Train one model per patient (train on others, test on this one)\n",
                "    model = create_rf_model()\n",
                "    model.fit(X_tr, y_tr)\n",
                "\n",
                "    y_hat = model.predict(X_te)\n",
                "\n",
                "    # Collect pooled (micro) predictions\n",
                "    pooled_y_true.extend(y_te.tolist())\n",
                "    pooled_y_pred.extend(y_hat.tolist())\n",
                "\n",
                "    # Per-patient metrics (recording-level within that patient)\n",
                "    acc = accuracy_score(y_te, y_hat)\n",
                "    bacc = balanced_accuracy_score(y_te, y_hat)\n",
                "    f1m = f1_score(y_te, y_hat, average=\"macro\")\n",
                "    n_imp = int(np.sum(np.array(y_te) == \"Impaired\"))\n",
                "    n_hea = int(np.sum(np.array(y_te) == \"Healthy\"))\n",
                "\n",
                "    rows.append(\n",
                "    {\n",
                "    \"Patient\": pid,\n",
                "    \"n_recordings\": int(test_mask.sum()),\n",
                "    \"n_Healthy\": n_hea,\n",
                "    \"n_Impaired\": n_imp,\n",
                "    \"Acc\": acc,\n",
                "    \"BalancedAcc\": bacc,\n",
                "    \"MacroF1\": f1m,\n",
                "    }\n",
                " )\n",
                "\n",
                "# ---- Aggregate results ----\n",
                "per_patient_df = pd.DataFrame(rows).sort_values(\"Patient\").reset_index(drop=True)\n",
                "\n",
                "# Macro over patients = mean of per-patient metrics (each patient = 1 unit)\n",
                "macro_over_patients = {\n",
                " \"Acc\": per_patient_df[\"Acc\"].mean(),\n",
                " \"BalancedAcc\": per_patient_df[\"BalancedAcc\"].mean(),\n",
                " \"MacroF1\": per_patient_df[\"MacroF1\"].mean(),\n",
                "}\n",
                "\n",
                "# Micro (pooled) across all LOPO predictions = compute once over all recordings\n",
                "pooled_y_true = np.array(pooled_y_true)\n",
                "pooled_y_pred = np.array(pooled_y_pred)\n",
                "\n",
                "micro_acc = accuracy_score(pooled_y_true, pooled_y_pred)\n",
                "micro_bacc = balanced_accuracy_score(pooled_y_true, pooled_y_pred)\n",
                "micro_f1m = f1_score(pooled_y_true, pooled_y_pred, average=\"macro\")\n",
                "\n",
                "print(\"=== LOPO (recording-level, arm labels) ===\")\n",
                "print(f\"Patients: {len(unique_pids)} | Total recordings: {len(pooled_y_true)}\")\n",
                "print(\"\\n-- Micro (pooled across all LOPO folds) --\")\n",
                "print(f\" Accuracy : {micro_acc:.3f}\")\n",
                "print(f\" BalancedAccuracy: {micro_bacc:.3f}\")\n",
                "print(f\" Macro F1 : {micro_f1m:.3f}\")\n",
                "\n",
                "print(\"\\n-- Macro over patients (mean of per-patient metrics) (I would deem this more relevant) --\")\n",
                "print(f\" Accuracy : {macro_over_patients['Acc']:.3f}\")\n",
                "print(f\" BalancedAccuracy: {macro_over_patients['BalancedAcc']:.3f}\")\n",
                "print(f\" Macro F1 : {macro_over_patients['MacroF1']:.3f}\")\n",
                "\n",
                "print(\"\\nPer-patient (recording-level) metrics:\")\n",
                "print(per_patient_df.to_string(index=False))\n",
                "\n",
                "# Optional: quick class-wise pooled report\n",
                "print(\"\\nPooled classification report (across all LOPO predictions):\")\n",
                "print(classification_report(pooled_y_true, pooled_y_pred, digits=3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f5a715e3",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
